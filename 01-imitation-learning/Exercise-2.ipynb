{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "Reinforcement Learning Course, 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import gym\n",
    "\n",
    "import expert # local file to import expert controllers\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run pipenv inside project folder:\n",
    "\n",
    "  > pipenv install\n",
    "\n",
    "  > pipenv shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def perform_rollout(env, controller, render=False, to_cpu=None, **controller_kwargs):\n",
    "    if to_cpu:\n",
    "        controller.cpu()\n",
    "    obs = env.reset()\n",
    "    traj_obs = []\n",
    "    traj_acts = []\n",
    "    traj_rewards = []\n",
    "    for i in range(5000): # maximal task horizon (anyway limited by env)\n",
    "        action = controller.predict(obs, **controller_kwargs)  # ask the controller (expert or network)\n",
    "        if isinstance(action, tuple): # the expert policy returns a tuple (action, some_internals)\n",
    "            action=action[0]\n",
    "        traj_obs.append(obs)\n",
    "        traj_acts.append(action)\n",
    "        obs, reward, done, info = env.step(action)  # make a step in the environment\n",
    "        traj_rewards.append(reward)\n",
    "        if render:\n",
    "            env.render()\n",
    "        if done:  # this is the sign that the rollout ended\n",
    "            break\n",
    "    traj = {'observations' : np.asarray(traj_obs),\n",
    "            'actions' : np.asarray(traj_acts),\n",
    "            'rewards' : np.asarray(traj_rewards)\n",
    "            }\n",
    "    return sum(traj_rewards), traj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Training-data and Behavioral Cloning\n",
    "\n",
    "load/create an environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env_name = \"LunarLanderContinuous-v2\"\n",
    "# env = gym.wrappers.TimeLimit(gym.make(env_name),1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or the walker env (but us less rollouts here, otherwise slow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"BipedalWalker-v3\"\n",
    "env = gym.wrappers.TimeLimit(gym.make(env_name),1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Run Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "expert_controller = expert.load_expert(f\"experts/ppo_{env_name}.zip\", device=device)\n",
    "number_runs = 20 # 50\n",
    "trajectories = []\n",
    "rewards = []\n",
    "for r in range(number_runs):\n",
    "    cum_reward, traj = perform_rollout(env, expert_controller, render=False, deterministic=True)\n",
    "    # TODO: Complete here\n",
    "    if cum_reward >= 100:\n",
    "        trajectories.append(traj)\n",
    "        rewards.append(cum_reward)\n",
    "    \n",
    "    # consider ignoring the rollouts where the expert is not good (reward < 100)\n",
    "    \n",
    "# make sure you keep the trajectories in \"trajectories\"\n",
    "# and all the rewards for plotting\n",
    "\n",
    "# if the rewards are in \"rewards\" you can print it this way\n",
    "print(f\"Expert performance {np.mean(rewards)} +- {np.std(rewards)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## convert trajectories into training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_all_data_from_trajectories(trajectories):\n",
    "    all_observations = np.concatenate([ t['observations'] for t in trajectories])\n",
    "    all_actions = np.concatenate([ t['actions'] for t in trajectories])    \n",
    "    print(f\"{len(trajectories)} trajectories loaded: {len(all_observations)} transitions\")\n",
    "    return all_observations, all_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert into training data for pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "expert_data = get_all_data_from_trajectories(trajectories)\n",
    "num_data_points = len(expert_data[0])\n",
    "# we split train and validation data (the last 10% of all data)\n",
    "num_train = int(num_data_points * 0.9)\n",
    "# conversion to PyTorch Tensors\n",
    "x_train = torch.FloatTensor(expert_data[0][0:num_train])\n",
    "y_train = torch.FloatTensor(expert_data[1][0:num_train])\n",
    "x_validation = torch.FloatTensor(expert_data[0][num_train:])\n",
    "y_validation = torch.FloatTensor(expert_data[1][num_train:])\n",
    "# it is handy to use the DataLoader class that helps us to sample batches for training\n",
    "train_dataset = TensorDataset(x_train, y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Define neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Feedforward(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(Feedforward, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes  = hidden_sizes\n",
    "        self.output_size  = output_size\n",
    "        \n",
    "        # TODO: complete here\n",
    "        self.input = torch.nn.Linear(input_size, self.hidden_sizes[0])\n",
    "        self.input_relu = torch.nn.ReLU()\n",
    "\n",
    "        self.hidden_layers = torch.nn.ModuleList()\n",
    "        for i in range(len(hidden_sizes)-1):\n",
    "            self.hidden_layers.append(torch.nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            self.hidden_layers.append(torch.nn.ReLU())\n",
    "        \n",
    "        # the last layer (readout) shoud not contain a non-linearity: here is already the code for it:\n",
    "        self.readout = torch.nn.Linear(self.hidden_sizes[-1], self.output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: write this\n",
    "        x = self.input(x)\n",
    "        x = self.input_relu(x)\n",
    "\n",
    "        for hidden in self.hidden_layers:\n",
    "            x = hidden(x)\n",
    "        return self.readout(x)\n",
    "\n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            return self.forward(torch.from_numpy(x.astype(np.float32))).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Setup the network and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = Feedforward(x_train.shape[1], [100,50], y_train.shape[1])\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.005, eps=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Validation error before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.eval()  # put model in evaluation mode\n",
    "with torch.no_grad():\n",
    "    y_pred = model(x_validation)\n",
    "    before_train = criterion(y_pred, y_validation)\n",
    "    print('Validation error before training' , before_train.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.train() # put model in training mode\n",
    "epochs = 10 # try more\n",
    "\n",
    "training_losses = []\n",
    "validation_losses = []\n",
    "for epoch in range(epochs):\n",
    "    for i, data in enumerate(train_dataloader, 0):  # Sample a batch from the data and perform training\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        pred = model(inputs)\n",
    "        # Compute Loss\n",
    "        loss = criterion(pred, labels)\n",
    "        # compute gradients and update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Todo: evaluate training and validation errors and store them\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(x_train)\n",
    "        train = criterion(y_pred, y_train)\n",
    "        training_losses.append(train.item())\n",
    "\n",
    "        y_pred = model(x_validation)\n",
    "        validation = criterion(y_pred, y_validation)\n",
    "        validation_losses.append(validation.item())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Todo: create a plot of the error over epochs\n",
    "plt.plot(validation_losses, color='red', label='validation loss')\n",
    "plt.plot(training_losses, color='blue', label='training loss')\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.title(env_name)\n",
    "plt.legend()\n",
    "# and save it to\n",
    "plt.savefig(f\"{env_name}_bc_training.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Test network as policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: run the policy as controller for 50 rollouts for the LunarLander (20 or Walker)\n",
    "# store rewards of all rollouts to do statistics later\n",
    "model_rewards = []\n",
    "for r in range(20):\n",
    "    cum_reward, traj = perform_rollout(env, model, render=False) \n",
    "    model_rewards.append(cum_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# consider making a box-plot with the rewards of the cloned network vs the expert\n",
    "plt.boxplot([model_rewards, rewards], labels=['model', 'expert'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Dagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Dagger let us put all pieces into small functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout_policy_and_collect_data(env, policy, number_rollouts, render=False):\n",
    "    # TODO: write this function: it should run the policy for number_rollouts times and return\n",
    "    rewards = []\n",
    "    trajectories = []\n",
    "    for r in range(number_runs):\n",
    "        cum_reward, traj = perform_rollout(env, model, render, True) \n",
    "        rewards.append(cum_reward)\n",
    "        trajectories.append(traj)\n",
    "    return np.asarray(rewards), trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relabel_actions(data, expert): # data is tuple of (all_observations, all_actions)\n",
    "    # TODO: use the expert to create new action labels for the given data. Return the relabeled data\n",
    "    relabeled_data = []\n",
    "    new_actions = np.zeros(data[1].shape)\n",
    "    \n",
    "    for i, o in enumerate(data[0]):\n",
    "        new_action = expert.predict(o)\n",
    "        # print(new_action)\n",
    "        new_actions[i] = new_action[0]\n",
    "        \n",
    "    return (data[0], new_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_policy(model, train_dataloader, num_epochs, criterion, optimizer, device):\n",
    "    # TODO: train the model/policy for the given number of epochs\n",
    "    if device == 'cuda':\n",
    "        model.cuda()\n",
    "    model.train() # put model in training mode    \n",
    "    \n",
    "    training_losses = []\n",
    "    validation_losses = []\n",
    "    for epoch in range(epochs):\n",
    "        for i, data in enumerate(train_dataloader, 0):  # Sample a batch from the data and perform training\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            pred = model(inputs)\n",
    "            # Compute Loss\n",
    "            loss = criterion(pred, labels)\n",
    "            # compute gradients and update\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_to_trainloader(data):    \n",
    "    x_train = torch.FloatTensor(data[0])\n",
    "    y_train = torch.FloatTensor(data[1])    \n",
    "    train_dataset = TensorDataset(x_train, y_train)\n",
    "    return DataLoader(train_dataset, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dagger -  Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dagger_performance = []\n",
    "#prefill data with expert data\n",
    "data = expert_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Feedforward(x_train.shape[1], [100,50], y_train.shape[1])\n",
    "model.cuda()\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.005, eps=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_dagger_iter = 50 # 100\n",
    "number_rollouts_per_iter = 5  # For the walker you can use 1 rollout only\n",
    "number_training_epochs_per_iter = 5\n",
    "\n",
    "for i in range(num_dagger_iter):\n",
    "    #TODO: implement the main Dagger Loop\n",
    "    print(f\"Iteration {i}, train on: {len(data[0])}\")\n",
    "    \n",
    "    train_dataloader = data_to_trainloader(data)\n",
    "    train_policy(model, train_dataloader, number_training_epochs_per_iter, criterion, optimizer, device)\n",
    "    # rollout policy \n",
    "    rewards, trajectories = rollout_policy_and_collect_data(env, model, number_rollouts_per_iter)\n",
    "    dagger_performance.append(np.mean(rewards))\n",
    "    \n",
    "    # relabel data    \n",
    "    relabeled_data = relabel_actions(get_all_data_from_trajectories(trajectories), expert_controller)\n",
    "\n",
    "    # merge_data (already done for you)\n",
    "    data = np.concatenate([data[0],relabeled_data[0]]), np.concatenate([data[1],relabeled_data[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: plot mean performance per dagger iteration \n",
    "plt.plot(dagger_performance, label='dagger mean performance per iter')\n",
    "plt.legend()\n",
    "plt.savefig(f\"{env_name}_dagger_training.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look a the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation (no rendering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dagger_rewards, _ = rollout_policy_and_collect_data(env, model, number_rollouts=50, render=False)\n",
    "np.mean(dagger_rewards),np.std(dagger_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put all three performance in a single boxplot (expert, bc, dagger)\n",
    "plt.boxplot([rewards, model_rewards, dagger_rewards], labels=['expert', 'bc', 'dagger'])\n",
    "\n",
    "plt.savefig(f\"{env_name}_result.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.12 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "676de112f8e726518f77207bf915c1a135e53a2ab05ca8c815c047b41e1c5cff"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}